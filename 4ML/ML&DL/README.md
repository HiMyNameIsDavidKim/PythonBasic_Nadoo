# ML & DL

## `[ML]`
* 경험을 통해 자동으로 개선하는 컴퓨터 알고리즘.
    * 알고리즘을 사용하여 데이터에서 패턴을 찾는다.
    * 간단하게 정의하면 머신 랭귀지로 러닝시키는 것.
* 인풋(데이터셋)을 주고, 익명의 함수(람다)를 사용하며, 아웃풋(정답지)를 알려준다.
* 종류 : 회귀(Regression) 분류(Classification) 트리(Tree) 비지도(Unsupervised)
    * 회귀(regression) : 임의의 숫자를 예측. 연속성 결과, 시퀀셜 결과. (ex. 고객 별 연체 확률 예측, 상품 판매량 예측)
    * 분류(classification) : 클래스 중 하나로 분류. 이산성 결과, 카테고리컬 결과. (ex. 이중분류_라쿤인가?, 다중분류_어떤동물?)
* 분야 : 1.CV(이미지 분류, 스캔, 게임), 2.NLP(분류, 요약, 이해, 수익예측, 음성인식, 구매이력)<br><br>

### [기본 개념]
* 라이브러리로 사이킷런을 사용.
* 람다 함수 : 익명 함수, 고차함수, 클로저, 콜백과 같은 개념.
* 클로저 : 환경을 담아놓고, 호출 시 꺼내 사용하는 함수.
* 입력 변수(X) : 샘플 = row 1개 = 확률 변수
* 출력 변수(y) : 클래스들 = 답안지 = 타겟 변수 = 기댓값
* 예측값(E) : 들어올 데이터에 대한 정확도. (y는 학습시킨 데이터에 대한 정확도.)
    * y = aX + b 에서 y가 출력변수, X가 입력변수.
    * 편의상 여기서 a를 계수나 가중치, b를 절편이나 바이어스라고 부른다.
* 모델링 : 시스템적 특성을 수식화하는 과정.
    * 시스템의 변화 예측이 방정식으로 표현된다. (미분방정식 or 확률함수)
* 지도 학습 : 훈련 데이터로부터 하나의 함수를 유추해내는 학습법.
    * 지도 학습은 입력변수와 출력변수의 값이 주어진 상태에서 러닝하는 것.
    * (ex. Regression, Classification)
* 비지도 학습 : 입력값에 대한 정답 없이 데이터 구성을 알아내는 학습법.
    * 비지도 학습은 입력변수만 있는 상태에서 러닝하는 것.
* 강화 학습 : 현재 상태에서 어떤 행동이 최적인지 보상을 통해 알아내는 학습법.
    * 강화 학습은 입력변수와 보상이 있는 상태에서 러닝하는 것. <br><br>

### [회귀 알고리즘]
* Regression, 연속성 결과를 예측하는 알고리즘.
* R^2 : 결정계수, 회귀 알고리즘을 평가할 때 사용. 정확한 숫자를 맞출 수 없기 때문에 사용함.
    * 사이킷런의 score() 메서드.
    * R^2 = 1 - (∑ (타깃 - 예측)^2 / ∑ (타깃 - 평균)^2)
* overfitting : train set에서 점수가 좋았으나 test set에서 점수가 아주 나쁜 경우.
* underfitting : train set에서 점수 보다 test set에서 점수가 더 높은 경우.
* regularization : 규제, train set을 너무 과도하게 overfit하지 않도록 방해하는 작업.
    * 릿지 회귀와 랏쏘 회귀에서 사용된다.
    * alpha로 규제하는 양을 조절 가능하다.
    * alpha의 적절값 : train 과 test의 R^2가 가장 가까운 경우.
* 종류 : k-최근접이웃 회귀, 선형 회귀, 다항 회귀, 다중 회귀, 릿지 회귀, 라쏘 회귀<br><br>
    
### [회귀 알고리즘의 종류]
* k-최근접이웃 회귀 : 근처 위치에 있는 데이터 k개 값을 기준으로 연속성 결과를 유추하는 알고리즘.
    * 사이킷런 KNeighborsRegressor() 클래스.
    * 한계 : train set의 range를 넘어서는 데이터가 들어오면 유추 불가.
* 선형 회귀 : 특성과 관련된 직선형 방정식(1차 방정식)을 학습하는 알고리즘.
    * 사이킷런 LinearRegression() 클래스.
    * 한계 : 직선 형태로만 설명이 가능하다. (곡선 불가)
* 다항 회귀 : 특성과 관련된 곡선형 방정식(다항 1차 방정식)을 학습하는 알고리즘.
    * 선형 회귀 + np.column_stack 메서드를 활용.
* 다중 회귀 : 여러개의 특성과 관련된 방정식(n차 방정식)을 학습하는 알고리즘.
    * 직선을 넘어 평면이나 큐브 형태의 답을 얻을 수 있다.
    * feature engineering : 기존의 특성으로 새로운 평면을 정의해 활용.
    * 사이킷런 PolynomialFeatures() 클래스로 feature 늘리기.
    * 이걸 그대로 사이킷런 LinearRegression() 클래스에 넣기.
    * 한계 : feature 수 증량을 많이 할수록 overfitting될 수 있음.
* 릿지 회귀 : 다중 회귀에 regularization 기법을 사용한 알고리즘.
    * 사이킷런 Ridge() 클래스.
    * 랏쏘와 다른 점은 규제에 의한 계수를 0으로 만들지는 않는다.
* 랏쏘 회귀 : 다중 회귀에 regularization 기법을 사용한 알고리즘.
    * 사이킷런 Lasso() 클래스.
    * 랏쏘와 다른 점은 규제에 의한 계수를 0으로 만들 수 있다.<br><br>

### [분류 알고리즘]
* Classifier, 이산성 결과를 예측하는 알고리즘.
* 각 클래스별 확률 예측 결과 : predict_proba() 메서드.
* 시그모이드 함수와 softmax 함수
    * 시그모이드 : '하나'의 선형방정식 결과를 0~1 사이로 압축. (=이진분류)
    * softmax : '여러개'의 선형방정식 결과를 0~1 사이로 압축. (=다중분류) 지수 함수의 특징을 사용해서 계산한다.
* epoch : 트레이닝 셋을 총 사용한 횟수.
* 손실 함수 : 1개 샘플에 대하여 알고리즘이 얼마나 틀렸는지 측정하는 기준 함수.
* 비용 함수 : 모든 샘플에 대한 알고리즘이 얼마나 틀렸는지 측정하는 기준 함수.
* 종류 : k-최근접이웃 분류, 로지스틱 회귀, 
* <br><br>

### [분류 알고리즘의 종류]
* k-최근접이웃 분류 : 근처 위치에 있는 데이터 k개 값을 기준으로 다중 분류 결과를 유추하는 알고리즘.
    * 사이킷런 KNeighborsClassifier() 클래스.
* 로지스틱 회귀 : 직선형 방정식(1차 방정식)을 학습하고 로지스틱 함수로 다중 분류 결과를 유추하는 알고리즘.
    * 회귀모델로 각 클래스의 확률으로 0~1 사이의 값을 유추하지만(회귀), 마지막에 로지스틱 함수를 곱하면 분류 알고리즘이 된다.
    * 사이킷런 LogisticRegression() 클래스.
    * 로지스틱 함수 : 시그모이드 함수 중 하나, 특징은 0~1 사이 값을 가진다.
    * 시그모이드 함수 : 기울어진 S자 형태의 곡선 함수. '매끄러운 연속 계단 함수'. 종류로 로지스틱, 아크 탄젠트, 하이퍼볼릭 탄젠트 등이 있다.
* 경사 하강법 : 
    * 점진적 학습 : 앞서 훈련한 모델을 쓰면서 새로운 데이터를 조금씩 추가로 훈련하는 방법.
    * 확률적 경사 하강법 : 기존 샘플 1개와 새로운 데이터로 최대 그라디언트 계산.
    * 미니배치 경사 하강법 : 미니배치와 새로운 데이터로 최대 그라디언트 계산.
* <br><br>



## `[DL]`
* 머신러닝(ML)의 한 종류로, 신경망(NN, Neural Network)을 수많은 계층 형태로 연결한 기법.<br><br>

### [기본 개념]
* 라이브러리로 텐서플로우, 파이토치를 사용.
    * 텐서플로우 : 구글의 DL 라이브러리.
    * 케라스를 인수합병 했다.
    * TensorFlow.js와 TensorFlow Lite가 있어서 모바일에도 적용 가능.
    * 파이토치 : 페이스북의 DL 라이브러리.
    * 더 파이써닉한 접근방식으로 람다를 적극 활용한다.
* 인공신경망 : ANN, Artificial Neural Network.
* 필터 : 뉴런 갯수. 가중치의 집합.
* 커널 : 가중치 1개.
* 윈도우 : 필터의 생김새. (n, n) 인지를 나타냄.
* 특성맵 : 원래 행렬을 필터와 합성곱 계산한 결과물.
* 패딩 : 합성곱 계산을 위해 외곽 테두리에 0인 패딩을 채운다.
* 스트라이드 : 커널의 이동 크기.
* 풀링 : 특성맵의 가로세로 크기를 줄이는 역할. 최대 혹은 평균 사용.<br><br>



