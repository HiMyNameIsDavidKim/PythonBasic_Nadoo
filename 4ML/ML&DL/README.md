# ML & DL

## `[데이터 전처리]`
* ML, DL 시 데이터를 사전 가공하는 메서드 정리.
* 맷플릿롭은 feature 간 스케일 차이를 이해하지 못한다.
* 시그마와 평균에 맞게 표준점수화(z-score) 시켜주는게 좋다.
    * (자동)
    * 사이킷런의 StandardScaler() 클래스.
    * ss.fit()
    * ss.transform()
    * (수동)
    * mean = np.mean(train_input, axis=0)
    * std = np.std(train_input, axis=0)
    * test_scaled = (test_input - mean) / std
* 트레이닝셋과 테스트셋을 나누어 평가한다. (8:2)
    * train_test_split() 메서드.
* 트레이닝셋에서 검증셋을 분리해 중간 평가를 반영한다. (6:2:2)
    * train_test_split() 메서드 한번 더 사용.
<br><br>

## `[ML]`
* 경험을 통해 자동으로 개선하는 컴퓨터 알고리즘.
    * 알고리즘을 사용하여 데이터에서 패턴을 찾는다.
    * 간단하게 정의하면 머신 랭귀지로 러닝시키는 것.
* 인풋(데이터셋)을 주고, 익명의 함수(람다)를 사용하며, 아웃풋(정답지)를 알려준다.
* 종류 : 회귀(Regression) 분류(Classification)
* 응용 : 트리(Tree) 비지도(Unsupervised)
    * 회귀(regression) : 임의의 숫자를 예측. 연속성 결과, 시퀀셜 결과. (ex. 고객 별 연체 확률 예측, 상품 판매량 예측)
    * 분류(classification) : 클래스 중 하나로 분류. 이산성 결과, 카테고리컬 결과. (ex. 이중분류_라쿤인가?, 다중분류_어떤동물?)
* 분야 : 1.CV(이미지 분류, 스캔, 게임), 2.NLP(분류, 요약, 이해, 수익예측, 음성인식, 구매이력)
<br><br>

### [기본 개념]
* 라이브러리로 사이킷런을 사용.
* 람다 함수 : 익명 함수, 고차함수, 클로저, 콜백과 같은 개념.
* 클로저 : 환경을 담아놓고, 호출 시 꺼내 사용하는 함수.
* 입력 변수(X) : 샘플 = row 1개 = 확률 변수
* 출력 변수(y) : 클래스들 = 답안지 = 타겟 변수 = 기댓값
* 예측값(E) : 들어올 데이터에 대한 정확도. (y는 학습시킨 데이터에 대한 정확도.)
    * y = aX + b 에서 y가 출력변수, X가 입력변수.
    * 편의상 여기서 a를 계수나 가중치, b를 절편이나 바이어스라고 부른다.
* 모델링 : 시스템적 특성을 수식화하는 과정.
    * 시스템의 변화 예측이 방정식으로 표현된다. (미분방정식 or 확률함수)
* 지도 학습 : 훈련 데이터로부터 하나의 함수를 유추해내는 학습법.
    * 지도 학습은 입력변수와 출력변수의 값이 주어진 상태에서 러닝하는 것.
    * (ex. Regression, Classification)
* 비지도 학습 : 입력값에 대한 정답 없이 데이터 구성을 알아내는 학습법.
    * 비지도 학습은 입력변수만 있는 상태에서 러닝하는 것.
* 강화 학습 : 현재 상태에서 어떤 행동이 최적인지 보상을 통해 알아내는 학습법.
    * 강화 학습은 입력변수와 보상이 있는 상태에서 러닝하는 것. 
<br><br>



## `[회귀 알고리즘]`
* Regression, 연속성 결과를 예측하는 알고리즘.
* R^2 : 결정계수, 회귀 알고리즘을 평가할 때 사용. 정확한 숫자를 맞출 수 없기 때문에 사용함.
    * 사이킷런의 score() 메서드.
    * R^2 = 1 - (∑ (타깃 - 예측)^2 / ∑ (타깃 - 평균)^2)
* overfitting : train set에서 점수가 좋았으나 test set에서 점수가 아주 나쁜 경우.
* underfitting : train set에서 점수 보다 test set에서 점수가 더 높은 경우.
* regularization : 규제, train set을 너무 과도하게 overfit하지 않도록 방해하는 작업.
    * 릿지 회귀와 랏쏘 회귀에서 사용된다.
    * alpha로 규제하는 양을 조절 가능하다.
    * alpha의 적절값 : train 과 test의 R^2가 가장 가까운 경우.
* 종류 : k-최근접이웃 회귀, 선형 회귀, 다항 회귀, 다중 회귀, 릿지 회귀, 라쏘 회귀
<br><br>
    
### [회귀 알고리즘의 종류]
* k-최근접이웃 회귀 : 근처 위치에 있는 데이터 k개 값을 기준으로 연속성 결과를 유추하는 알고리즘.
    * 사이킷런 KNeighborsRegressor() 클래스.
    * 한계 : train set의 range를 넘어서는 데이터가 들어오면 유추 불가.
* 선형 회귀 : 특성과 관련된 직선형 방정식(1차 방정식)을 학습하는 알고리즘.
    * 사이킷런 LinearRegression() 클래스.
    * 한계 : 직선 형태로만 설명이 가능하다. (곡선 불가)
* 다항 회귀 : 특성과 관련된 곡선형 방정식(다항 1차 방정식)을 학습하는 알고리즘.
    * 선형 회귀 + np.column_stack 메서드를 활용.
* 다중 회귀 : 여러개의 특성과 관련된 방정식(n차 방정식)을 학습하는 알고리즘.
    * 직선을 넘어 평면이나 큐브 형태의 답을 얻을 수 있다.
    * feature engineering : 기존의 특성으로 새로운 평면을 정의해 활용.
    * 사이킷런 PolynomialFeatures() 클래스로 feature 늘리기.
    * 이걸 그대로 사이킷런 LinearRegression() 클래스에 넣기.
    * 한계 : feature 수 증량을 많이 할수록 overfitting될 수 있음.
* 릿지 회귀 : 다중 회귀에 regularization 기법을 사용한 알고리즘.
    * 사이킷런 Ridge() 클래스.
    * 랏쏘와 다른 점은 규제에 의한 계수를 0으로 만들지는 않는다.
* 랏쏘 회귀 : 다중 회귀에 regularization 기법을 사용한 알고리즘.
    * 사이킷런 Lasso() 클래스.
    * 랏쏘와 다른 점은 규제에 의한 계수를 0으로 만들 수 있다.
<br><br>



## `[분류 알고리즘]`
* Classifier, 이산성 결과를 예측하는 알고리즘.
* 각 클래스별 확률 예측 결과 : predict_proba() 메서드.
* epoch : 트레이닝 셋을 총 사용한 횟수.
* max_iter : epoch와 동일한 용어.
* mini batch : 트레이닝 셋을 조각냈을 때 하나의 조각.
* batch size : 미니 배치가 갖는 데이터의 수.
* 점진적 학습 : 앞서 훈련한 모델을 쓰면서 새로운 데이터를 조금씩 추가로 훈련하는 방법. 경사 하강법으로 구현 가능.
* partial_fit : 에포크 1회 추가 학습.(점진적 학습)
* early stopping : 과대적합 전에 미리 훈련을 멈추는 방법.
* 종류 : k-최근접이웃 분류, 로지스틱 회귀, SGD 분류.
<br><br>

### [분류 알고리즘의 종류]
* k-최근접이웃 분류 : 근처 위치에 있는 데이터 k개 값을 기준으로 다중 분류 결과를 유추하는 알고리즘.
    * 사이킷런 KNeighborsClassifier() 클래스.
* 로지스틱 회귀 : 직선형 방정식(1차 방정식)을 학습하고 로지스틱 함수로 다중 분류 결과를 유추하는 알고리즘.
    * 회귀모델로 각 클래스의 확률으로 0~1 사이의 값을 유추하지만(회귀), 마지막에 로지스틱 함수를 곱하면 분류 알고리즘이 된다.
    * 사이킷런 LogisticRegression() 클래스.
* SGD 분류 : 확률적 경사 하강법을 사용한 분류 알고리즘.
    * 사이킷런 SGDClassifier() 클래스.
<br><br>



## `[트리 알고리즘]`
* Tree, 순차적인 조건을 나무처럼 엮은 알고리즘.
* 회귀와 분류가 모두 가능하다.
* 표준화 전처리(StandardScaler)를 할 필요 없다.
* 불순도 : 여러 개의 클래스가 섞여 있는 정도를 표현하는 척도.
    * criterion : 표준, 표준화할 방식을 결정.
    * 지니계수 : 공평하게 섞여 있는지 나타내는 지표.
    * 크로스 엔트로피 : 무질서한 정도를 수치화한 지표.
    * 지니계수는 2차방정식 기반이고 크로스 엔트로피는 로그함수 기반. 곡률이 미세하게 다르지만 퍼포먼스에는 크게 차이가 없다.
    * <img width="425" alt="스크린샷 2022-12-28 오후 12 19 19" src="https://user-images.githubusercontent.com/112922638/209752094-d60957b2-cb91-45e6-bab5-541f328b1f86.png">
* 정보이득 : information gain, 부모와 자식 노드 사이의 불순도 차이.
* 종류 : 결정 트리, 랜덤 포레스트, 엑스트라 트리, 그라디언트 부스팅, 히스토그램 기반 그라디언트 부스팅(=XGBoost =LightGBM).
<br><br>

### [트리 알고리즘의 종류]
* 결정 트리 : 연속적인 질문(조건)을 이어가며 학습하는 알고리즘.
    * 사이킷런의 DecisionTreeClassifier() 클래스.
    * plot_tree() 함수로 시각화 가능.
    * 부모노드와 자식노드의 불순도 차이가 최대가 되도록 성장함.
    * 특성의 중요도 추출 가능. feature_importances_ 속성.
* 랜덤 포레스트 : 결정 트리의 앙상블 학습 알고리즘.
    * 사이킷런의 RandomForestClassifier() 클래스.
    * 앙상블 학습 중에 배깅 방식.
    * 100개의 결정트리를 사용.
    * 랜덤하게 데이터 샘플링하여 n개의 데이터셋 생성.
    * 노드 분할 시 최대의 정보이득인 경우를 선택.
* 엑스트라 트리 : 랜덤 포레스트와 유사한 알고리즘.
    * 사이킷런의 ExtraTreeClassifier() 클래스.
    * 앙상블 학습 중에 배깅의 변형 방식.
    * 전체 데이터셋을 사용.
    * 노드 분할 시 랜덤한 경우를 선택.
* 그라디언트 부스팅 : 깊이가 얕은 이진 결정 트리의 앙상블 학습 알고리즘.
    * 사이킷런의 GradientBoostingClassifier() 클래스.
    * 깊이 3의 결정트리를 사용. -> 일반화에 유리하다.
    * 결정트리를 계속 추가하며 경사하강법을 사용.
* 히스토그램 기반 그라디언트 부스팅 : 히스토그램 기법이 추가로 적용.
    * 트레이닝셋의 각 피쳐 컬럼으로 히스토그램을 그려 256개 구간으로 나누고 피쳐값을 255개의 구간 인덱스로 취급해버린다.
    * 그라디언트 계산이 간단해져 노드를 분할하는 속도가 빨라진다.
    * 머리로는 이해가 안되겠지만... 실제 계산해보면 (부모 - 왼쪽 자식 = 오른쪽 자식)이 성립한다...!
    * 정형 데이터 분석의 최강 알고리즘.
    * 사이킷런의 HistGradientBoostingClassifier() 클래스.
    * 타 라이브러리 XGBoost, LightGBM도 사용 가능.
<br><br>

### [교차검증]
* cross validation, 트레이닝셋에서 검증셋을 분리할 때 k개로 나눈 뒤 k번의 검증을 하는 검증법.
* 트레이닝셋에 과대 적합되어 일반화된 평가에서 약점이 생기는 것을 방지한다.
* cross_validate() 함수.

### [하이퍼파라미터 튜닝]
* 하이퍼파라미터 미세 변화에 따라 정확도가 달라진다. -> 마찬가지로 교차검증 대상
* grid search
    * 모든 파라미터를 조금씩 고치며 완전탐색 알고리즘으로 확인.
    * GridSearchCV() 클래스.
* random search
    * 파라미터 조건이 너무 많을 경우 확률 분포를 활용해 서치.
    * RandomSearchCV() 클래스.
<br><br>

### [앙상블 학습]
* ensemble learning, 여러개의 클래시파이어를 생성하고 모델을 조합하여 더 정확한 예측을 하는 학습법.
* 보팅 : voting, 각 모델의 결과로 투표를 통해 최종 예측 결과를 결정.
    * 다른 유형의 알고리즘
    * 같은 통합 데이터셋
* 배깅 : Bootstrap AGGregatING, 데이터 샘플링(bootstrap)을 통해 모델을 학습시키고 결과를 집계(Aggregating).
    * 같은 유형의 알고리즘
    * 다른 쪼개진 데이터셋
* 부스팅 : Boosting, 여러개의 클래시파이어가 순차적으로 학습하되 이전 알고리즘이 다음 알고리즘에게 가중치를 알려줌.
    * 같은 유형의 알고리즘
    * 같은 통합 데이터셋
    * 가장 정확하지만 가장 느리다
<br><br>



## `[비지도 학습]`
* Unsupervised learning, 타깃이 없을 때 사용하는 알고리즘.
* 
<br><br>



### [Scikit-learn 함수 사용 순서]
* 1.Classifier() or Regressor()
* 2.cross_validate()
* 3.fit()
* 4.scores()
* 5.save()
<br><br>



## `[DL]`
* 머신러닝(ML)의 한 종류로, 신경망(NN, Neural Network)을 수많은 계층 형태로 연결한 기법.
<br><br>

### [기본 개념]
* 라이브러리로 텐서플로우, 파이토치를 사용.
    * 텐서플로우 : 구글의 DL 라이브러리.
    * 케라스를 인수합병 했다.
    * TensorFlow.js와 TensorFlow Lite가 있어서 모바일에도 적용 가능.
    * 파이토치 : 페이스북의 DL 라이브러리.
    * 더 파이써닉한 접근방식으로 람다를 적극 활용한다.
* 인공신경망 : ANN, Artificial Neural Network.
* 필터 : 뉴런 갯수. 가중치의 집합.
* 커널 : 가중치 1개.
* 윈도우 : 필터의 생김새. (n, n) 인지를 나타냄.
* 특성맵 : 원래 행렬을 필터와 합성곱 계산한 결과물.
* 패딩 : 합성곱 계산을 위해 외곽 테두리에 0인 패딩을 채운다.
* 스트라이드 : 커널의 이동 크기.
* 풀링 : 특성맵의 가로세로 크기를 줄이는 역할. 최대 혹은 평균 사용.
<br><br>

### [딥러닝 레이어]
* 활성화 함수 : activation function, 입력된 데이터의 가중 합을 출력 신호로 변환하는 함수.
    * ReLU : 입력값이 0보다 작으면 0, 크면 y=x로 출력. 은닉층에 사용.
    * Leaky ReLU : 0보다 작은 노드를 완전히 죽이는 오류를 개선. 은닉층에 사용.
    * Sigmoid : 0~1 범위의 기울어진 S자 형태의 곡선 함수. 이진분류 출력층에 사용. 
        * (실제 시그모이드 함수란 S자 형태를 말하는 것이고, 0~1 사이 값을 가지는 시그모이드 함수는 로지스틱 함수이다. 하지만 관용적으로 동의어로 취급)
    * Softmax : 0~1 범위의 익스포넨셜 형태의 함수. 정규화 되어 출력의 총합이 1인 특징. 다중분류 출력층에 사용.
* 손실 함수 : loss function, 1개 샘플에 대하여 예측값과 실제 정답의 차이를 비교하는 함수.
    * BEE : 바이너리 크로스 엔트로피, 이진분류에 사용.
    * CEE : 카테고리컬 크로스 엔트로피, 다중분류에 사용.
    * MSE : 평균 제곱 오차, 회귀에 사용.
    * 비용 함수 : cost function, 모든 샘플에 대하여 예측값과 실제 정답의 차이를 비교하는 함수. 각 샘플에 대한 로스의 총합 = 코스트.
* 옵티마이저 : 최소의 loss로 학습하는 방법을 찾는 최적화 알고리즘.
    * SGD : 확률적 경사 하강법, 기존 샘플 1개와 새로운 데이터로 최대 그라디언트 계산하여 최대 속도로 학습.
    * RMSProp : 루트 민 스퀘어 예측법, 기울기에 따라 학습률을 조절해서 정확도 높임.
    * 모멘텀 : 경사 하강법에 관성 개념을 추가. 지역 최솟값에 갇히지 않도록 설계됨.
    * 아담 : RMSProp과 모멘텀의 하이브리드.
<br><br>

### [TensorFlow 함수 사용 순서]
* 1.sequential()
* 2.compile()
* 3.fit()
* 4.evaluate()
* 5.save()