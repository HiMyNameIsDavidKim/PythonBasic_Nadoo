# ML & DL

## `[ML]`
* 경험을 통해 자동으로 개선하는 컴퓨터 알고리즘.
    * 알고리즘을 사용하여 데이터에서 패턴을 찾는다.
    * 간단하게 정의하면 머신 랭귀지로 러닝시키는 것.
* 인풋(데이터셋)을 주고, 익명의 함수(람다)를 사용하며, 아웃풋(정답지)를 알려준다.
* 종류 : 회귀(Regression) 분류(Classification) 트리(Tree) 비지도(Unsupervised)
    * 회귀(regression) : 임의의 숫자를 예측. 연속성 결과, 시퀀셜 결과. (ex. 고객 별 연체 확률 예측, 상품 판매량 예측)
    * 분류(classification) : 클래스 중 하나로 분류. 이산성 결과, 카테고리컬 결과. (ex. 이중분류_라쿤인가?, 다중분류_어떤동물?)
* 분야 : 1.CV(이미지 분류, 스캔, 게임), 2.NLP(분류, 요약, 이해, 수익예측, 음성인식, 구매이력)
<br><br>

### [기본 개념]
* 라이브러리로 사이킷런을 사용.
* 람다 함수 : 익명 함수, 고차함수, 클로저, 콜백과 같은 개념.
* 클로저 : 환경을 담아놓고, 호출 시 꺼내 사용하는 함수.
* 입력 변수(X) : 샘플 = row 1개 = 확률 변수
* 출력 변수(y) : 클래스들 = 답안지 = 타겟 변수 = 기댓값
* 예측값(E) : 들어올 데이터에 대한 정확도. (y는 학습시킨 데이터에 대한 정확도.)
    * y = aX + b 에서 y가 출력변수, X가 입력변수.
    * 편의상 여기서 a를 계수나 가중치, b를 절편이나 바이어스라고 부른다.
* 모델링 : 시스템적 특성을 수식화하는 과정.
    * 시스템의 변화 예측이 방정식으로 표현된다. (미분방정식 or 확률함수)
* 지도 학습 : 훈련 데이터로부터 하나의 함수를 유추해내는 학습법.
    * 지도 학습은 입력변수와 출력변수의 값이 주어진 상태에서 러닝하는 것.
    * (ex. Regression, Classification)
* 비지도 학습 : 입력값에 대한 정답 없이 데이터 구성을 알아내는 학습법.
    * 비지도 학습은 입력변수만 있는 상태에서 러닝하는 것.
* 강화 학습 : 현재 상태에서 어떤 행동이 최적인지 보상을 통해 알아내는 학습법.
    * 강화 학습은 입력변수와 보상이 있는 상태에서 러닝하는 것. 
<br><br>

### [회귀 알고리즘]
* Regression, 연속성 결과를 예측하는 알고리즘.
* R^2 : 결정계수, 회귀 알고리즘을 평가할 때 사용. 정확한 숫자를 맞출 수 없기 때문에 사용함.
    * 사이킷런의 score() 메서드.
    * R^2 = 1 - (∑ (타깃 - 예측)^2 / ∑ (타깃 - 평균)^2)
* overfitting : train set에서 점수가 좋았으나 test set에서 점수가 아주 나쁜 경우.
* underfitting : train set에서 점수 보다 test set에서 점수가 더 높은 경우.
* regularization : 규제, train set을 너무 과도하게 overfit하지 않도록 방해하는 작업.
    * 릿지 회귀와 랏쏘 회귀에서 사용된다.
    * alpha로 규제하는 양을 조절 가능하다.
    * alpha의 적절값 : train 과 test의 R^2가 가장 가까운 경우.
* 종류 : k-최근접이웃 회귀, 선형 회귀, 다항 회귀, 다중 회귀, 릿지 회귀, 라쏘 회귀
<br><br>
    
### [회귀 알고리즘의 종류]
* k-최근접이웃 회귀 : 근처 위치에 있는 데이터 k개 값을 기준으로 연속성 결과를 유추하는 알고리즘.
    * 사이킷런 KNeighborsRegressor() 클래스.
    * 한계 : train set의 range를 넘어서는 데이터가 들어오면 유추 불가.
* 선형 회귀 : 특성과 관련된 직선형 방정식(1차 방정식)을 학습하는 알고리즘.
    * 사이킷런 LinearRegression() 클래스.
    * 한계 : 직선 형태로만 설명이 가능하다. (곡선 불가)
* 다항 회귀 : 특성과 관련된 곡선형 방정식(다항 1차 방정식)을 학습하는 알고리즘.
    * 선형 회귀 + np.column_stack 메서드를 활용.
* 다중 회귀 : 여러개의 특성과 관련된 방정식(n차 방정식)을 학습하는 알고리즘.
    * 직선을 넘어 평면이나 큐브 형태의 답을 얻을 수 있다.
    * feature engineering : 기존의 특성으로 새로운 평면을 정의해 활용.
    * 사이킷런 PolynomialFeatures() 클래스로 feature 늘리기.
    * 이걸 그대로 사이킷런 LinearRegression() 클래스에 넣기.
    * 한계 : feature 수 증량을 많이 할수록 overfitting될 수 있음.
* 릿지 회귀 : 다중 회귀에 regularization 기법을 사용한 알고리즘.
    * 사이킷런 Ridge() 클래스.
    * 랏쏘와 다른 점은 규제에 의한 계수를 0으로 만들지는 않는다.
* 랏쏘 회귀 : 다중 회귀에 regularization 기법을 사용한 알고리즘.
    * 사이킷런 Lasso() 클래스.
    * 랏쏘와 다른 점은 규제에 의한 계수를 0으로 만들 수 있다.
<br><br>

### [분류 알고리즘]
* Classifier, 이산성 결과를 예측하는 알고리즘.
* 각 클래스별 확률 예측 결과 : predict_proba() 메서드.
* epoch : 트레이닝 셋을 총 사용한 횟수.
* mini batch : 트레이닝 셋을 조각냈을 때 하나의 조각.
* batch size : 미니 배치가 갖는 데이터의 수.
* 점진적 학습 : 앞서 훈련한 모델을 쓰면서 새로운 데이터를 조금씩 추가로 훈련하는 방법. 경사 하강법으로 구현 가능.
* 종류 : k-최근접이웃 분류, 로지스틱 회귀, 
<br><br>

### [분류 알고리즘의 종류]
* k-최근접이웃 분류 : 근처 위치에 있는 데이터 k개 값을 기준으로 다중 분류 결과를 유추하는 알고리즘.
    * 사이킷런 KNeighborsClassifier() 클래스.
* 로지스틱 회귀 : 직선형 방정식(1차 방정식)을 학습하고 로지스틱 함수로 다중 분류 결과를 유추하는 알고리즘.
    * 회귀모델로 각 클래스의 확률으로 0~1 사이의 값을 유추하지만(회귀), 마지막에 로지스틱 함수를 곱하면 분류 알고리즘이 된다.
    * 사이킷런 LogisticRegression() 클래스.
<br><br>



## `[DL]`
* 머신러닝(ML)의 한 종류로, 신경망(NN, Neural Network)을 수많은 계층 형태로 연결한 기법.
<br><br>

### [기본 개념]
* 라이브러리로 텐서플로우, 파이토치를 사용.
    * 텐서플로우 : 구글의 DL 라이브러리.
    * 케라스를 인수합병 했다.
    * TensorFlow.js와 TensorFlow Lite가 있어서 모바일에도 적용 가능.
    * 파이토치 : 페이스북의 DL 라이브러리.
    * 더 파이써닉한 접근방식으로 람다를 적극 활용한다.
* 인공신경망 : ANN, Artificial Neural Network.
* 필터 : 뉴런 갯수. 가중치의 집합.
* 커널 : 가중치 1개.
* 윈도우 : 필터의 생김새. (n, n) 인지를 나타냄.
* 특성맵 : 원래 행렬을 필터와 합성곱 계산한 결과물.
* 패딩 : 합성곱 계산을 위해 외곽 테두리에 0인 패딩을 채운다.
* 스트라이드 : 커널의 이동 크기.
* 풀링 : 특성맵의 가로세로 크기를 줄이는 역할. 최대 혹은 평균 사용.
<br><br>

### [딥러닝 레이어]
* 활성화 함수 : activation function, 입력된 데이터의 가중 합을 출력 신호로 변환하는 함수.
    * ReLU : 입력값이 0보다 작으면 0, 크면 y=x로 출력. 은닉층에 사용.
    * Leaky ReLU : 0보다 작은 노드를 완전히 죽이는 오류를 개선. 은닉층에 사용.
    * Sigmoid : 0~1 범위의 기울어진 S자 형태의 곡선 함수. 이진분류 출력층에 사용. 
        * (실제 시그모이드 함수란 S자 형태를 말하는 것이고, 0~1 사이 값을 가지는 시그모이드 함수는 로지스틱 함수이다. 하지만 관용적으로 동의어로 취급)
    * Softmax : 0~1 범위의 익스포넨셜 형태의 함수. 정규화 되어 출력의 총합이 1인 특징. 다중분류 출력층에 사용.
* 손실 함수 : loss function, 1개 샘플에 대하여 예측값과 실제 정답의 차이를 비교하는 함수.
    * BEE : 바이너리 크로스 엔트로피, 이진분류에 사용.
    * CEE : 카테고리컬 크로스 엔트로피, 다중분류에 사용.
    * MSE : 평균 제곱 오차, 회귀에 사용.
    * 비용 함수 : cost function, 모든 샘플에 대하여 예측값과 실제 정답의 차이를 비교하는 함수. 각 샘플에 대한 로스의 총합 = 코스트.
* 옵티마이저 : 최소의 loss로 학습하는 방법을 찾는 최적화 알고리즘.
    * SGD : 확률적 경사 하강법, 기존 샘플 1개와 새로운 데이터로 최대 그라디언트 계산하여 최대 속도로 학습.
    * RMSProp : 루트 민 스퀘어 예측법, 기울기에 따라 학습률을 조절해서 정확도 높임.
    * 모멘텀 : 경사 하강법에 관성 개념을 추가. 지역 최솟값에 갇히지 않도록 설계됨.
    * 아담 : RMSProp과 모멘텀의 하이브리드.
<br><br>

### [TensorFlow 함수 사용 순서]
* 1.sequential()
* 2.compile()
* 3.fit()
* 4.evaluate()
* 5.save()