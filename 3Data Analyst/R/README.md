# R
* 통계, 데이터 마이닝, 그래프를 위한 언어.
* 🎉ADsP 합격 (2023.03.24)🎉
    * 강의 -> 기출(유튜브) -> 기출(풀기)
<br><br>

## `[R 프로그래밍 기본]`
* 통계, 데이터마이닝, 그래프를 위한 언어.
* R의 경쟁 프로그램인 SAS, SPSS는 유료이며 용량이 지나치게 크다.
* SQL UI에 파이썬으로 코딩하는 느낌...
* 특징
    * 오픈소스 프로그램
    * 뛰어난 그래픽 및 빠른 성능
    * 시스템 데이터 저장 방식 (RAM 기반)
    * 모든 운영체제 사용 가능
    * S라는 통계 언어 기반 표준 플랫폼
    * 객체지향 + 함수형
<br><br>

### [R 기초]
* 기본
    * $ : 요소 뽑아내기
    * dataframe : 서로 다른 타입의 데이터를 열마다 넣을 수 있다.
    * list : 서로 다른 타입의 데이터를 하나로 묶을 수 있다.
* matrix 함수
    * ncol = 컬럼 갯수.
    * 기본적으로 첫번째 컬럼을 우선적으로 채움. 세로쓰기.
    * byrow = TRUE 해주면 가로쓰기 가능.
* summary 함수
    * 연속형 : 최솟값, 1사분위수, 중간값, 평균, 3사분위수, 최댓값
    * 범주형 : (범주 : 범주별 데이터 개수)가 콜론으로 구분되어 표시
* 결측치 대치법
    * 단순 대치법 : 완전 응답 개체분석, 평균 대치법, 단순확률 대치법
    * 다중 대치법 : 추정량 표준오차의 과소추정, 계산의 난해성 문제
* 이상값(outlier)
    * 구하는 방법이 여러가지.
    * (평균 ± 3 * 표준편차) 밖의 값.
    * (Q1 - 1.5 * IQR), (Q3 + 1.5 * IQR) 밖의 값.
    * 무조건 삭제.
* IQR
    * InterQuartile Range, 사분위수 범위.
    * IQR = Q3 - Q1
* Box Plot
    * 이상치 존재 확인 가능.
    * IQR 길이, 최소, 최대, 1사분위, 3사분위, 중위값 확인 가능.
    * NA 제거하고 그려짐.
<br><br>



## `[데이터 이해]`
* 데이터 : 라틴어인 dare의 과거분사형으로 주어진 것이라는 의미.
* 데이터는 추론과 추정의 근거를 이루는 사실이다.
<br><br>

### [데이터 유형]
* 유형
    * 정성적 데이터 : 언어나 문자로 표현. (ex. 직업, 기상특보)
    * 정량적 데이터 : 수치, 도형, 기호 등 약속으로 표현. (ex. 나이, 몸무게)
* 지식 경영
    * 암묵지 : 개인에게 체화. 겉으로 드러나지 않는 지식.
    * 형식지 : 책, 문서, 메뉴얼. 형상화된 지식.
    * 공통화 -> 표출화 -> 연결화 -> 내면화
    * 공통화 : 암묵지를 타인에게 전파.
    * 표출화 : 암묵지를 책같은 형식지로 제작.
    * 연결화 : 책같은 형식지에 자신의 새로운 지식 추가.
    * 내면화 : 책같은 형식지를 보고 타인들이 암묵지 습득.
* 구조적 유형
    * 정형 데이터 : 행과 열 구별 입력. 스키마 및 메타 데이터 특성. RDBMS로 관리.
    * 반정형 데이터 : 행과 열 구별 입력 X. 스키마 및 메타 데이터 특성.
    * 비정형 데이터 : 음원, 이미지, 동영상 등 데이터 프레임의 형태가 없는 데이터.
* DIKW 피라미드
    * Data : 개별 데이터 자체로는 의미가 없는 객관적 사실.
    * Information(정보) : 데이터 가공. 데이터간 의미 도출.
    * Knowledge(지식) : 도출 정보를 구조화하여 분류. 재조합. 고유의 지식을 내재화.
    * Wisdom(지혜) : 지식의 축적. 아이디어 결합. 창의적 산물.
* 데이터 저장 방식
    * RDBMS : Oracle, MSSQL, MySQL
    * NoSQL : MongoDB, HBase, Redis, Cassandra
    * 분산파일 시스템 : HDFS
* 크기순서 : Kilo - Mege - Giga - Tera - Peta - Exa - Zeta - Yota
<br><br>

### [데이터베이스]
* 체계적이거나 조직적으로 정리되고 개별적으로 접근할 수 있는 독립된 데이터.
* 동시에 복수의 적용 업무를 지원할 수 있도록 데이터를 받고 저장.
* 문자, 기호, 음성, 화상, 영상 등 빅데이터로 의미가 확장되고 있다.
* (50년 미국 기지), (75년 우리나라 최초 사용), (80년 이후 적극 사용)
* 특징
    * Integrated, 자료의 중복이 없다.
    * Stored, 저장되어 있다.
    * Shared, 여러 사람이 이용한다.
    * Changeable, 항상 업데이트 된다.
* DB 설계 절차
    * 요구조건 분석
    * 개념적 설계
    * 논리적 설계
    * 물리적 설계
<br><br>

### [경영정보 시스템]
* OLAP(분석, 대화식)
* OLTP(트랜젝션)
* Data Mining(데이터의 관계, 규칙, 패턴을 찾는 과정)
* CRM(Customer Relationship Management)(고객 특성 기반 마케팅 활동 계획, 지원, 평가)
* SCM(Supply Chain Management)(공급망, 생산, 유통과정 관리)
* ERP(Enterprise Resource Planning)(전사 경영활동 통합 관리)
* RTE(Real Time Enterprise)(실시간, 의사결정 속도, 제품 수명)
* BI(Business Intelligence)(DW에 접근해 의사결정을 위한 정보 획득)
* EAI(Enterprise Application Integration)(경영정보 시스템 들을 상호 연동 및 통합)
* EDW(Enterprise Data Warehouse)(리소스의 유기적 통합, 데이터 중복 방지)
* KMS(Knowledge Management System)(지적 재산 관리)
<br><br>

### [산업별 분석 활용]
* 금융 서비스 : 신용점수, 사기 탐지, 가격 책정, 프로그램 트레이딩, 클레임 분석, 수익성 분석
* 병원 : 가격 책정, 고객 로열티, 수익 관리
* 에너지 : 트레이딩, 공급 수요 예측
* 정부 : 사기 탐지, 사례 관리, 범죄 방지, 수익 최적화
<br><br>

### [빅데이터]
* 대용량 데이터를 활용해 소용량에서 얻을 수 없었던 새로운 통찰이나 가치를 추출.
* 3V
    * Volume(양, 규모 측면)
    * Velocity(속도, 데이터 수집과 처리 측면)
    * Variety(다양성, 데이터 유형과 소스 측면)
    * 4V는 Varacity 추가
    * 5V는 Value 추가
    * 7V는 (Validation, Volatility) 추가
* 출현 배경
    * 클라우드 컴퓨팅이 경제적 효과를 제공
    * 양질 전환
    * 비정형 데이터의 등장
    * 처리기술의 발달
* 빅데이터 가치 산정의 어려움
    * 데이터 활용 방식(재사용이나 재조합이 가능하고, 누가 쓰는지에 따라 다름)
    * 새로운 가치 창출(빅데이터가 창출하는 가치를 포함해 산정해야함)
    * 분석 기술의 발전(지금은 분석 불가능 하더라도 미래 새로운 분석기술 등장)
* 위기요인
    * 사생활 침해(위치정보, 구매정보 등)
    * 책임 원칙 훼손(예측 알고리즘의 희생양)
    * 데이터 오용(예측 알고리즘의 지나친 믿음, 잘못된 지표 사용)
* 정의와 범주의 변화
    * 데이터 변화 : 3V가 커짐.
    * 기술 변화 : 데이터 처리, 저장, 분석 기술 발전.
    * 인력,조직 변화 : DS와 같은 인재와 조직이 필요.
<br><br>

### [빅데이터 활용 기본 테크닉(알고리즘)]
* 연관 규칙 학습
    * 피쳐 간 주목할 상관관계 탐색.
    * (ex. 커피를 구매하면 탄산음료를 많이 구매하는가?)
* 유형 분석
    * 특성에 따라 그룹을 분류.
    * (ex. 사용자는 어떤 특성의 집단에 속하는가?)
* 유전자 알고리즘
    * 자연선택, 돌연변이. 점진적 진화를 유도.
    * (ex. 최대 시청률을 위한 프로그램 종류와 시간 배치는?)
* 기계 학습
    * 트레이닝셋으로 학습한 특성을 통해 예측.
    * (ex. 기존 시청기록을 바탕으로한 다음 영화 추천은?)
* 회귀 분석
    * 독립변수 조작에 따른 종속변수의 관계를 파악.
    * (ex. 구매자의 나이가 구매 물품의 타입에 영향을 미치는가?)
* 감정 분석
    * 말하거나 글 쓴 사람의 감정을 분석.
    * (ex. 새로운 환불 정책에 대한 고객의 평가는?)
* 소셜 네트워크 분석
    * 사람 간 몇촌 정도의 관계인지 파악.
    * (ex. 고객 간 관계망의 구성은?)
<br><br>

### [데이터 사이언스]
* 데이터로부터 의미 있는 정보를 추출해내는 학문.
* 데이터 분석뿐만 아니라 이를 효과적으로 구현하고 전달하는 과정까지 포함한다.
* 구성 요소
    * 데이터 분석(수학, 확률모델, 머신러닝, 분석학, 모델링 등)
    * IT(시그널 프로세싱, 프로그래밍, 엔지니어링, 고성능 컴퓨팅 등)
    * 비즈니스 분석(커뮤니케이션, 프레젠테이션, 스토리텔링, 시각화 등)
* 하드 스킬
    * 빅데이터 이론 지식과 방법론
    * 분석 기술에 대한 숙련도
* 소프트 스킬
    * 창의력, 호기심, 논리적 비판
    * 스토리텔링, 비주얼라이제이션
    * 커뮤니케이션
<br><br>



## `[데이터 분석 기획]`
* 과제 정의에서부터 결과 도출까지의 과정을 적절히 관리할 수 있도록 사전에 계획하는 작업.
* 분석은 분석의 대상(What), 분석의 방법(How)에 따라 4가지 유형으로 나뉜다.
    * 방법 = known, 대상 = known -> Optimization
    * 방법 = unknown, 대상 = known -> Solution
    * 방법 = known, 대상 = unknown -> Insight
    * 방법 = unknown, 대상 = unknown -> Discovery
<br><br>

### [분석 방법론]
* 데이터 분석을 효율적으로 하기 위해서 데이터 분석 방법론(프로세스 정립)이 필요하다.
* 일정한 수준의 품질, 누구나 동일한 산출물, 간단한 지식만으로도 활용 가능 해야한다.
* 구성
    * 절차(Procedures)
    * 방법(Method)
    * 도구와 기법(Tools & Techniques)
    * 템플릿과 산출물(Templates & Outputs)
* 합리적 의사결정 장애요소 : 고정관념, 편향된 생각, 프레이밍 효과
    * 긍정적 : 비판적 사고, 직관력, 비편향적 사고
* 종류 : KDD, CRISP-DM, 빅데이터
<br><br>

### [KDD 분석 방법론]
* Knowledge Discovery in Database.
* 데이터를 통해 통계적 패턴이나 지식을 찾을 수 있도록 정리한 프로세스.
* 절차
    * 데이터셋 선택 : Selection, 비즈니스 이해, 목표 설정, 데이터 선택, 데이터셋 생성, 타깃 데이터 구성
    * 데이터 전처리 : Preprocessing, 노이즈, 아웃라이어, 미싱 벨류 처리
    * 데이터 변환 : Transformation, 변수 생성, 변수 선택, 차원 축소
    * 데이터 마이닝 : Data Mining, 데이터 마이닝 기법 선택, 알고리즘 선택, 데이터 마이닝 실행
    * 결과 평가 : Interpretation and Evaluation, 결과 해석, 평가, 분석 목적과 일치성 확인
<br><br>

### [CRISP-DM 분석 방법론]
* Cross Industry Standard Process for Data Mining
* (단계, 일반 과제, 세부 과제, 프로세스 실행) 4가지 레벨로 구성된 계층적 프로세스.
* 절차
    * 비즈니스 이해 : Business Understanding
    * 데이터 이해 : Data Understanding, (KDD의 Selection + Preprocessing)
    * 데이터 준비 : Data Preparation, (KDD의 Transformation)
    * 모델링 : Modelling, (모델 평가 포함)
    * 평가 : Evaluation, (분석 결과 평가, 모델링 과정 평가, 모델 적용성 평가 포함)
    * 배포 : Deployment
<br><br>

### [빅데이터 분석 방법론]
* 기존 분석 모델에서 빅데이터로 확장하여 분석하기 위한 모델.
* (단계, 태스크, 스텝) 3가지 레벨로 구성된 계층적 프로세스.
* 절차
    * 분석 기획 : Planning
    * 데이터 준비 : Preparing
    * 데이터 분석 : Analyzing
    * 시스템 구현 : Developing
    * 평가 및 전개 : Deploying
* 위험 대응 방법 : 회피(avoid), 전이(Transfer), 완화(Mitigate), 수용(Accept)
<br><br>

### [분석과제 발굴 방법론]
* 하향식, 상향식, 점증적 3가지로 나뉜다.
* 하향식 접근 방식
    * Top Down Approach
    * 폭포수 모델
    * 솔루션을 찾기 위해 체계적으로 단계화된 과정을 수행하는 방식. 문제 탐색.
    * 분석 대상(What)이 known 으로, Optimization -> Solution 순서로 진행.
* 상향식 접근 방식
    * Bottom Up Approach
    * 프로토타입 모델
    * 문제 정의 자체가 어려운 경우. 데이터 기반 문제 재정의 및 해결방안 탐색을 반복. 고객 이야기가 나옴.
    * 분석 대상(What)이 unknown 으로, Discovery -> Insight 순서로 진행.
* 점증적 개발 방식
    * 나선형 모델
    * 반복을 통해 점증적으로 개발.
    * 복잡성 증가할 수 있으므로 경계.
* Design Thinking : 중요 의사결정 시 상향식과 하향식을 반복적으로 사용.
<br><br>

### [분석과제 관리]
* 다른 프로젝트 처럼 범위, 일정, 품질, 리스크, 의사소통 관리가 필요하다.
* 데이터에 기반한 분석 기법을 적용하므로, 5대 주요 영역도 고려하여 관리가 필요하다.
* 5대 주요 영역
    * Data Complexity : 데이터의 복잡도
    * Data Size : 데이터의 크기
    * Speed : 분석을 뽑는 주기
    * Analytic Complexity : 분석법의 복잡도
    * Accuracy & Precision : 정확도와 일관성
* 여기서 복잡도와 정확도는 트레이드 오프 관계.
<br><br>

### [분석 마스터플랜 수립]
* 분석 마스터플랜 : 말그대로 분석에 대한 계획.
* ISP 방법론 활용, 데이터 분석 기획 특성 고려, 과제 우선순위 결정, 단기 및 중장기로 나누어 수립.
    * ISP(Information Strategy Planning)
    * 우선순위 고려 요소 : 전략적 중요도, 비즈니스 성과, ROI, 실행 용이성
    * 적용범위 고려 요소 : 업무 내재화 적용 수준, 분석 데이터 적용 수준, 기술 적용 수준
<br><br>

### [분석 거버넌스 체계]
* 거버넌스 = 관리
* 기업에서 데이터가 어떻게 관리, 유지, 규제되는지에 대한 내부적 관리 방식이나 프로세스.
* 구성요소
    * System : 분석 관련 IT 기술 및 프로그램.
    * Data Governance : 데이터 거버넌스.
    * HR(Human resource) : 분석교육, 마인드육성 체계.
    * Organization : 분석기획 및 관리 수행 조직.
    * Process : 과제 기획 및 운영 프로세스.
<br><br>

### [데이터 거버넌스]
* 거버넌스 = 관리
* 전사 차원의 모든 데이터에 대하여 정책, 지침, 운영조직 등 표준화된 관리 체계를 수립하고 운영을 위한 프레임워크(FW) 및 저장소(Repository)를 구축하는 것.
* 주요 관리 대상 : 마스터 데이터, 메타 데이터, 데이터 사전.
* 구성요소
    * 데이터 표준화
    * 데이터 관리체계
    * 데이터 저장소 관리
    * 표준화 활동
* 특징
    * 데이터 가용성, 유용성, 통합성, 보안성, 안정성 확보.
    * 독자적 or 전사 차원의 IT 거버넌스나 EA의 구성요소로 구축.
    * 빅데이터 효율적 관리, 데이터 최적화, 데이터 생명주기 관리.
    * 원칙(Principle), 조직(Organization), 프로세스(Process) 관리.
    * ERD는 지속적으로 변경사항을 관리.
<br><br>

### [분석 성숙도]
* 도입 단계 : 분석 시작. 환경, 시스템 구축.
* 활용 단계 : 분석 결과를 업무에 적용.
* 확산 단계 : 전사 공유.
* 최적화 단계 : 분석 진화, 혁신, 성과 기여.
<br><br>

### [추가 용어]
* Servitization : 제조업과 서비스업의 융합 용어.
* CoE(Center of Excellence) : 조직 내 분석 전문조직.
* Sandbox : 보안모델. 외부 접근 및 영향을 차단하여 제한된 영역에서만 동작.
* CMMI : 능력 성숙도 통합 모델. 분석에 대한 성숙도를 평가하기 위한 모델.
<br><br>



## `[데이터 분석 기법]`
* 데이터 처리 과정
    * 기존 운영시스템(Legacy)이 데이터 수집.
    * 스테이징, 운영 데이터 저장소(ODS)에서 데이터 클렌징.
    * 데이터 웨어하우스에 데이터 적제. (업로드만 가능)
    * 데이터 마트로 이동. (부서의 데이터 웨어하우스)
    * R, SAS로 시각화
* 시각화 : 가장 낮은 수준이지만 효율적. 시각화는 필수.
* 공간분석 : 공간 관련 데이터를 지도 위에 생성.
* 탐색적 자료 분석(EDA) : 다양한 차원과 값을 조합하여 의미있는 사실 도출.
    * 4가지 주제(저향성의 강조, 잔차 계산, 자료변수의 재표현, 그래프)
<br><br>

### [데이터 웨어하우스]
* 특징
    * 주제지향적
    * 통합적
    * 시계열
    * 비휘발성
    * 업로드만 가능(지속 갱신 아님)
<br><br>

### [데이터 마트]
* 데이터 웨어하우스와 사용자 사이의 중간층에 위치.
* 하나의 부서 중심의 데이터 웨어하우스라고 할 수 있다.
* RDB를 데이터 웨어하우스라고 하면, 필요한 데이터를 데이터 마트(마케팅부서, 재무부서, 등)로 복사해서 보낸다.
<br><br>



## `[통계 분석]`
* 모집단 -> (추출) -> 표본
* 표본의 통계량 -> (추론) -> 모집단의 모수
* 모집단 : population, 데이터 전체 집합.
* 모수 : parameter, 모집단의 특성을 나타내는 수치.(ex. 모평균, 모분산 등)
* 표본 : sample, 모집단에서 일부를 추출한 것.
* 통계량 : statistic, 표본의 특성을 나타내는 수치.(ex. 표본평균, 표본분산 등)
<br><br>

### [표본 추출]
* 단순 무작위 추출 : 표본으로 선택될 확률이 모두 동일.
* 계통 추출 : 일정 간격으로 표본 선택. (ex. 1, 11, 21, 31 ...)
* 층화 추출 : 모집단을 서로 겹치지 않게 층으로 나누고, 각 집단에서 원하는 수만큼 단순 무작위 추출.
* 군집 추출 : 차이가 없는 여러 개의 집단(클러스터)로 나눔.
<br><br>

### [척도의 종류]
* 명목 척도 : norminal. 순위가 없는 범주형 데이터. (ex. 성별)
* 서열 척도(순위) : ordinal. 순위가 있는 범주형 데이터. (ex. 금메달, 선호도)
* 등간 척도(구간) : interval. 절대적 원점이 없는 수치형 데이터. (ex. 물가지수)
* 비율 척도 : ratio. 절대적 원점이 있는 수치형 데이터. (ex. 몸무게, 나이)
<br><br>

### [통계 용어]
* 편차 = 변량 - 평균
* 분산 = 편차 제곱 합 / n-1
* 표준편차 = 루트 분산
* 집중화 경향
    * 왼쪽으로 치우쳐진
    * 오른쪽 꼬리가 긴 
    * 오른쪽에 mean이 있다.
    * Mode < Median < Mean
* 확률 : 사건이 일어날 수 있는 가능성을 수로 나타낸 것.
    * A가 발생할 확률은 P(A)이다.
    * 확률 = 사건/표본공간
    * 확률 값 : 0~1 사이의 값.
<br><br>

### [사건의 종류]
* 독립 사건 : A의 발생이 B가 발생할 확률을 바꾸지 않는 사건.
    * 조건부확률 P(A|B) = P(A)
    * 교집합 P(A∩B) = P(A)P(B)
* 배반 사건 : 한쪽이 일어나면 다른 쪽이 일어나지 않는 사건.
    * 교집합 P(A∩B) = 0
    * 합집합 P(A∪B) = P(A) + P(B)
* 종속 사건 : 한 사건의 결과가 다른 사건에 영향을 주는 사건.
    * 교집합 P(A∩B) = P(A|B)P(B)
* 조건부 확률 : 사건 B가 발생했다는 조건 아래서 사건 A가 발생할 확률.
    * 조건부 확률 P(A|B) = P(A∩B)/P(B)
<br><br>

### [확률 분포]
* 분포 : 일정한 범위 안에 흩어져 퍼져 있는 정도.
* 확률변수 : 확률 현상에 기인해 결과값이 확률적으로 정해지는 변수.
* 확률분포 : 어떤 확률변수가 취할 수 있는 모든 값들과 그때 확률을 대응관계로 표시하는 것.
* 이산형 확률 분포
    * 이항분포, 베르누이분포, 기하분포, 포아송분포
    * 기댓값 E(x) = Σ (x * f(x))
* 연속형 확률 분포
    * 정규분포, 지수분포, 연속균일분포, 카이제곱분포, F분포
    * 기댓값 E(x) = ∫ (x * f(x))
<br><br>

### [확률 분포의 특징]
* 베르누이 분포
    * 실험 결과가 둘 중 하나인 확률변수의 확률분포.
    * P(X=0) = p, P(X=1) = q, q = 1-p
    * 기댓값 E(x) = p
    * 분산 V(x) = p*q
* 이항 분포
    * 베르누이 시행을 n번 반복할 때, 성공한 x의 확률분포.
    * n = 1일 때 베르누이 분포와 같다.
    * 기댓값 E(x) = np
    * 분산 V(x) = np(1-p)
* 기하 분포
    * 베르누이 시행에서 처음 성공까지 시도한 횟수 x의 확률분포.
* 포아송 분포
    * 단위 시간이나 공간에서 어떤 사건이 몇 번 발생할 것인지를 표현하는 확률분포.
* 표준 정규 분포
    * μ = 0, σ = 1인 정규 분포(z분포).
    * z분포의 평균±표준편차 에서 1배 = 68%, 2배 = 95%, 3배 = 99.7%
    * 중심 극한 정리 : n이 충분히 크면 표본평균은 정규분포를 따른다. 모집단의 분포와 상관 없이 표본의 크기가 30 이상이면 표본평균의 분포가 정규분포에 근사해진다.
* T 분포
    * 정규분포는 표본의 수가 적으면 신뢰도가 낮다.
    * 표본을 많이 못뽑으면 T분포를 사용한다.
    * 표본의 갯수에 따라 모양이 변한다.
* 카이제곱 분포
    * 분산의 특징을 확률분포로 만든 것.
    * 자유도에 따라 모양이 바뀐다.
* F 분포
    * 분산의 특징을 확률분포로 만든 것.
    * 카이제곱은 한 집단의 분산. F 분포는 두 집단의 분산.
    * 두 집단의 분산이 서로 같은지 다른지 판단하기 위해 사용.
* 관계도
    * 정규분포 -> (표준화) -> 표준 정규 분포
    * 표준 정규 분포 -> (n이 작고 분산 모름) -> T 분포
    * 표준 정규 분포 -> (분산을 앎) -> 카이제곱 분포(한집단)
    * 표준 정규 분포 -> (분산을 앎) -> F 분포(두집단)
<br><br>

### [통계적 추정]
* 모집단 가정 여부별 종류
    * 모수적 추론 : 모집단에 특정 분포를 가정하고 모수에 대한 추론 진행.
    * 비모수적 추론 : 모집단에 대해 특정 분포 가정을 하지 않음.
* 추론 목적별 종류
    * 추정 : 통계량을 사용하여 모집단의 모수를 구체적으로 추측하는 과정.
        * 점 추정 : 하나의 값으로 모수의 값이 얼마인지 추측.
        * 구간 추정 : 모수를 포함할 것으로 기대되는 구간을 확률적으로 계산.
    * 가설 검정 : 모수에 대한 가설을 세우고, 가설을 확률적으로 판정하는 방법.
<br><br>

### [표준 오차]
* 표준 편차 : 한 표본에서 전체 개체가 가지는 값들의 차이에 대한 값.
* 표준 오차
    * SE(Standard Error)
    * 모집단의 샘플을 무한번 뽑은 각 샘플마다 평균들의 표준편차
    * SE = σ/√(n)
    * n이 클수록 작아진다.(=데이터가 많으면 정확해진다)
* 오차 한계
    * 표본 오차. 추정에서 추정구간 중심에서 최대 허용 오차.
    * Z값 * SE
<br><br>

### [좋은 추정량 판단 기준]
* 일치성
    * consistency
    * 표본의 크기가 커짐에 따라 표본 오차가 작아져야 한다.
* 비편향성
    * unbiasedness
    * bias가 없어야한다.
    * 추정량의 기댓값이 모수의 값과 같아야 한다.
* 효율성
    * 추정량의 분산이 최대한 작아야한다.
    * 최소 분산 추정량
    * MSE가 작아야 한다.
<br><br>

### [통계적 추정의 특징]
* 점 추정
    * 모수가 특정한 값 하나일 것이라고 추정하는 방법.
    * 적률법 : 표본의 기댓값을 통해 모수를 추정.
    * 최대가능도추정법 : 최대우도법, 손실함수를 미분해서 기울기가 0인 위치를 찾는 방법.
    * 최소제곱법 : 오차를 제곱한 합이 최소가 되는 함수를 구하는 방법.
* 구간 추정
    * 신뢰구간을 만들어서 추정하는 방법.
    * 신뢰구간 : 모수가 포함되리라고 기대하는 범위.
    * 신뢰수준 : 신뢰귀간 중 모수값을 포함하는 신뢰구간이 존재할 확률.
    * (ex. 신뢰수준 95%에서 투표자의 35~45%가 A 후보를 지지한다.)
    * 신뢰도가 높아지면 신뢰구간도 길어진다.
    * 표본의 크기가 커지면 신뢰구간의 길이는 줄어든다.
* 가설 검정
    * 가설을 설정하고 표본관찰을 통해 가설의 채택 여부를 결정하는 통계적 추론 방법.
    * 귀무가설 : 증명된 바 없는 가설. (기각하고자 하는 가설)
    * 대립가설 : 새로운 주장인 가설. (채택하고자 하는 가설)
    * α : 유의수준, 귀무가설이 실제 참임에도 기각할 확률.
    * β : 귀무가설이 거짓임에도 채택할 확률.
    * 1종 오류 : 귀무가설이 참임에도 기각하는 오류. (=α)
    * 2종 오류 : 귀무가설이 거짓임에도 채택하는 오류. (=β)
    * 유의 확률 : p value, 1종 오류를 범할 확률. (0.05 미만 추구)
    * 유의 확률이 0.05 미만일 때 귀무가설 기각, 대립가설 채택
<br><br>

### [모수적 추론]
* 모집단에 특정 분포를 가정하고 모수에 대해 추론하는 방법.
* 전제 조건 : 정규분포를 이루어야 하며, 집단 내 분산은 같아야 함.
* 종류 : T-test 3개, 평균관련(z분포, t분포), 분산관련(F분포)
* 자유도 = 표본의 갯수 - 1
* T-test
    * 평균값이 올바른지 두 집단의 평균 차이를 t값으로 검증.
    * 1 샘플 T-test : 단일 표본의 평균 검정.
    * 대응표본 T-test : 동일 개체에 처리 전 후 비교, 동일 특성 두 개체에 서로 다른 처리.
    * 독립표본 T-test : 서로 다른 두 그룹의 평균 검정.
<br><br>

### [정규성 검정]
* Q-Q plot
    * 그래프를 그려 시각적으로 대각선인지 확인.
* Histogram
    * 히스토그램을 그려 시각적으로 정규분포인지 확인.
* S-W test
    * p-value가 0.05 이상이면 정규성을 가정.
* K-S test
    * p-value가 0.05 이상이면 정규성을 가정.
<br><br>

### [비모수적 추론]
* 모집단에 가정이 없고, 주로 분포 형태에 관한 추론을 함.
* 종류 : 카이제곱 검정, 사인 테스트, (+길고 복잡한 이름들)
<br><br>

### [회귀 분석]
* 독립변수 = 입력값, 종속변수 = 출력값
* 단순 선형 회귀 = 한개의 독립변수
* 다중 선형 회귀 = 두개 이상의 독립변수
* 회귀 모형의 가정
    * 선형성 : 선형 모형이다.
    * 독립성 : 잔차와 독립변수는 관련이 없다.
    * 정규성 : 정규분포를 따른다.
    * 등분산성 : 잔차의 분포는 동일한 분산을 갖는다.
    * 비상관성 : 잔차끼리 상관이 없다.
* 통계적 의미를 갖는가? p-value는 0.05 보다 적어야 한다. F 통계량 확인.
* 모델이 얼마나 설명력이 있는가? R^2이 64%를 넘어야 한다.
<br><br>

### [회귀분석 용어]
* 다중 공산성 : 피쳐들 간의 상관성이 존재함. 높으면 회귀계수의 분산을 증가시켜 문제가 됨.
* L1 norm : 맨해튼 거리. 절대값의 합. Lasso.
* L2 norm : 유클리드 거리. 피타고라스. Ridge.
* 정규화 : 값의 범위를 0~1이 되도록 변환.
* 표준화 : 특성 값이 정규분포를 갖도록 변환.
<br><br>

### [상관분석 용어]
* 스피어만 상관계수 : 서열척도. 비선형적 관계. -1~1 사이. 상관없으면 0.
* 피어슨 상관계수 : 등간척도. 공분산을 표준편차의 곱으로 나눈 값. -1~1 사이.
<br><br>

### [시계열 용어]
* 시계열 자료의 정상조건 : 평균과 분산은 모든 시점에 일정, 공분산은 시점에 의존하지 않고 시차에만 의존.
* AR 모델 : 과거의 자료로 설명 가능한 모델.
* MA 모델 : 항상 정상성을 만족하는 모델.
* ARIMA 모델 : 항상 비정상성을 띄는 모델. 차분이나 변환으로 AR, MA, ARMA로 정상화 가능.
* ARIMA(p, d, q)에서 p : AR모형 차수, d : 차분, q : MA모형 차수.
<br><br>



## `[데이터 마이닝]`
* 모든 사용가능한 원천 데이터를 기반으로 감춰진 지식, 기대하지 못했던 경향 또는 새로운 규칙 등을 발견하고 이를 실제 비즈니스 의사결정 등에 유용한 정보로 사용.
* 데이터 마이닝 5단계
    * 목적 정의
    * 데이터 준비(클리닝, 양 확보)
    * 데이터 가공(목적변수 정의, SW에 맞게 가공, 개발환경 구축)
    * 데이터 마이닝 기법 적용
    * 검증
<br><br>

### [대표적인 종류]
* 분류
    * Classification
    * 기존에 정의된 집합에 배정.
* 군집
    * Clustering
    * 레코드와의 유사성에 의해 그룹화, 이질성에 의해 세분화.
* 연관분석
    * Association Analysis
    * 아이템들의 연관성을 파악하는 분석.
* 추정
    * Estimation
    * 알려지지 않은 결과의 값을 추정.
* 예측
    * Prediction
    * 미래 예측
* 기술
    * Description
    * 데이터가 가진 특징 및 의미를 단순히 설명.
<br><br>

### [분류]
* 로지스틱 회귀 분석
    * probability는 0~1 사이의 값.
    * odds는 확률에 대해 0~무한 으로 변환. (=로지스틱의 회귀계수)
    * log odds는 전체 실수 범위로 확장. (=logit)
    * sigmoid는 다시 0~1 사이 값으로 바꾸는 함수.
* 의사결정나무
    * 분리 기준 : 순수도가 높아지고 불확실성이 낮아지는 방향으로 분리.
    * 정지 규칙 : 분리가 일어나지 않고 최종 마디가 되도록 하는 규칙.
    * 가지 치기 규칙 : 최종 노드가 너무 많으면 오버피팅 가능성이 높다.
    * 지니지수 계산 : 1-∑(한 클래스 샘플수/전체수)^2
    * CART : 이산형에서는 지니지수를 쓰고, 연속형에서는 분산 감소량을 쓴다.
    * CHAID : 이산형에서는 카이제곱, 연속형에서는 아노바를 쓴다.
* 앙상블 모델
    * 여러개의 분류 모형에 의한 결과를 종합하여 분류의 정확도를 높인다.
    * 약하게 학습된 여러 모델들을 결합하여 사용.
    * 성능을 분산시키므로 과적합 감소 효과가 있다.
<br><br>

### [인공신경망 모델]
* 장점
    * 이상치 노이즈에 대해서도 민감하게 반응하지 않는다.
    * 입력변수와 출력변수가 연속형, 이산형을 가리지 않고 처리 가능.
* 단점
    * 결과에 대한 해석이 쉽지 않다.
    * 훈련 과정에 시간이 많이 소요된다.
    * 정규화를 하지 않으면 지역해에 빠질 수 있다.
* 활성화 함수
    * 결과를 내보낼 때 사용하는 함수.
    * 시그모이드 or 소프트맥스.
* 문제 발생
    * 은닉층의 노드가 너무 적으면 언더피팅 위험.
    * 은닉층의 노드가 너무 많으면 오버피팅 위험.
    * 역전파 알고리즘 : 동일 입력층에 대해 원하는 값이 출력되도록 각 웨이트를 조절하는 방법.
    * 기울기 소실 문제 : 은닉층이 너무 많아 역전파 알고리즘이 잘 안되는 문제.
* 모델 평가
    * 홀드 아웃 : 원천 데이터를 트레이닝셋과 테스트셋으로 분리.
    * 교차 검증 : k개로 분리, 1개 빼고 트레이닝셋으로 사용, 결과를 평균냄.
    * 부트 스트랩 : 랜덤하게 샘플링. 한 샘플이 여러번 선정될 수도 있음.
    * 0.632 부트 스트랩 기억.
<br><br>

### [오분류표 평가지표]
* ![이미지 오분류표](https://user-images.githubusercontent.com/112922638/219871290-f83c56ad-0d37-43b8-9e06-918dcef0a34d.png)
* 정밀도
    * Precision(=Pre)
    * TP / (TP + FP)
* 재현율
    * Sensitivity(=Sen), Recall
    * TP / (TP + FN)
* F1
    * 2 * (Pre * Sen) / (Pre + Sen)
* 정분류율
    * Accuracy
    * (TP + TN) / (TP + FP + FN + TN)
* 오분류율
    * Error rate
    * (FP + FN) / (TP + FP + FN + TN)
* 특이도
    * Specificity
    * TN / (TN + FP)
* FP rate
    * FP / (TN + FP)
* 이해하기
    * TP = NG정합, TN = OK정합, FN = 미검, FP = 과검
    * True는 무조건 잘했다. False는 무조건 잘못했다.
    * Precision : NG판정 중에 NG정합(=과검 평가)
    * Recall : 실제NG 중에 NG정합(=미검 평가)
    * F1 : Pre와 Sen의 조화평균. OK와 NG 모두에 대해 높은 정밀도와 높은 재현율.
    * Accuracy : 전체 중에 정합
    * Error rate : 전체 중에 미검과검
<br><br>

### [분류모델 성능평가]
* ROC 커브
    * Receiver Operating Characteristic
    * X축은 FP rate, Y축은 재현율으로 그래프를 그려 성능 평가.
    * 위로 꺾인 곡선 꼴이 되면 최고 성능, 직선 꼴이 되면 최저 성능.
    * ROC 그래프의 밑부분의 면적(AUC)가 넓을 수록 좋음.
<br><br>

### [군집]
* 계층적 군집
    * Hierarchical Clustering
    * 두 개체간의 거리에 기반하므로 거리 측정 정의 필요.
    * 이상치에 민감함.
    * 사전에 군집수 k를 선택하지 않아도 된다.
    * 한번 군집이 형성되면 다른 군집으로 이동하지 않는다.
    * 종류 : 최단 연결법, 최장 연결법, 중심 연결법, 와드 연결법, 평균 연결법
    * 거리 : 수학적(유클리드, 맨해튼, 민코프스키), 통계적(표준화, 마할라노비스)
    * 유클리드는 피타고라스, 맨해튼은 절대값차의 합.
* 분할적 군집
    * 종류 : k-means, k-medians, k-메도이드, DBSCAN, EM 알고리즘
* SOM
    * Self-Organizing Maps, 자기 조직화 지도.
    * 비지도 인공신경망의 한 종류로 차원축소와 군집화를 동시에 수행.
    * 입력층과 경쟁층 단 2개의 층으로 이루어져 있다.
    * 경쟁하여 학습힌다. 역전파가 아니라 전방패스를 사용하여 매우 빠르다.
<br><br>

### [군집화 평가 지수]
* 실루엣 계수
    * 클러스터 안의 데이터들이 다른 클러스터와 비교해 얼마나 비슷한가를 평가.
    * 1에 가까울 수록 군집화가 잘 되었다. 0.5보다 크면 타당함.
<br><br>

### [연관 분석]
* 아이템들 간의 연관 규칙을 발견해 내는 방법.
* Apriori 알고리즘
    * 데이터들에 대한 발생 빈도를 기반으로 연관관계를 계산.
    * 데이터셋이 너무 크면 비효율적이다.
* FP Growth
    * Apriori 단점을 보완하기 위해 등장.
    * FP-tree와 node, link라는 특별한 자료구조를 사용.
* 장점
    * 결과를 이해하기 쉽다.
    * 분석 계산이 간편하다.
* 단점
    * 아이템의 수가 늘어나면 계산도 기하급수적으로 증가한다.
<br><br>

### [연관규칙 측정 지표]
* 지지도
    * Support
    * 전체 아이템 중 삼품 A와 상품 B를 동시에 포함하여 거래하는 비율.
    * 지지도 = P(A∩B)
* 신뢰도
    * Confidence
    * 상품 A를 포함하는 거래 중 A와 B가 동시에 거래되는 비율.
    * 신뢰도 = P(B|A) = P(A∩B) / P(A)
* 향상도
    * Lift
    * A가 주어지지 않았을 때 B의 확률 대비 A가 주어졌을 대 B의 확률 증가 비율.
    * 향상도 = P(B|A)/P(B) = P(A∩B) / (P(A)*P(B))
* 향상도가 1이면 두 아이템은 독립이다.
* 향상도가 1보다 클수록 연관성이 높다. 1보다 작으면 음의 상관관계.
<br><br>